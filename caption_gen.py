from __future__ import annotations

import json
import logging
import random
import re
import time
import unicodedata
from collections.abc import Iterable, Sequence
from dataclasses import dataclass
from datetime import date, datetime, timezone
from typing import TYPE_CHECKING, Any, Literal
from zoneinfo import ZoneInfo

from data_access import Asset
from openai_client import OpenAIClient

if TYPE_CHECKING:  # pragma: no cover
    from jobs import Job
    from main import Bot

POSTCARD_OPENING_CHOICES = (
    "–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –æ—Ç–∫—Ä—ã—Ç–æ—á–Ω—ã–º –≤–∏–¥–æ–º.",
    "–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –∫—Ä–∞—Å–∏–≤—ã–º –≤–∏–¥–æ–º",
)
POSTCARD_PREFIX = POSTCARD_OPENING_CHOICES[1]
POSTCARD_RUBRIC_HASHTAG = "#–æ—Ç–∫—Ä—ã—Ç–æ—á–Ω—ã–π–≤–∏–¥"
POSTCARD_DEFAULT_HASHTAGS = ["#–ë–∞–ª—Ç–∏–∫–∞", "#–ö—Ä–∞—Å–∏–≤—ã–π–í–∏–¥"]
POSTCARD_MIN_HASHTAGS = 3
POSTCARD_HASHTAG_LIMIT = 5
POSTCARD_BANNED_TAGS = {
    "#–∫–æ—Ç–æ–ø–æ–≥–æ–¥–∞",
    "#–ö–æ—Ç–æ–ø–æ–≥–æ–¥–∞",
    "#–ë–∞–ª—Ç–∏–π—Å–∫–æ–µ–ú–æ—Ä–µ",
    "#–±–∞–ª—Ç–∏–π—Å–∫–æ–µ–º–æ—Ä–µ",
}
POSTCARD_BANNED_TAG_KEYS = {tag.lstrip("#").casefold() for tag in POSTCARD_BANNED_TAGS}
POSTCARD_ADDITIONAL_STOP_PHRASES = (
    "–∫–∞–¥—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—á–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Å–Ω–æ–≤–∞",
    "–¥–µ–ª–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω—ã–º",
    "–Ω–∞–ø–æ–ª–Ω—è–µ—Ç –æ—Å–æ–±—ã–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º",
    "–º–æ–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—á–µ—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–≤—Å–µ–≥–¥–∞",
    "–∏–¥–µ–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ",
    "–∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω—ã–π",
    "–∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–∞—è",
    "–∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–æ–µ",
    "–º–∞–≥–∏—è –º–æ–º–µ–Ω—Ç–∞",
    "—É–Ω–∏–∫–∞–ª—å–Ω—ã–π",
    "—É–Ω–∏–∫–∞–ª—å–Ω–∞—è",
    "—É–Ω–∏–∫–∞–ª—å–Ω–æ–µ",
    "–æ—Å–æ–±–µ–Ω–Ω–æ–µ –º–µ—Å—Ç–æ, –≥–¥–µ —Ö–æ—á–µ—Ç—Å—è –æ—Å—Ç–∞—Ç—å—Å—è –Ω–∞–≤—Å–µ–≥–¥–∞",
    "—à–µ–ø—á–µ—Ç",
    "—à–µ–ø—Ç–∞–ª–∞",
    "—à–µ–ø—Ç–∞–ª–∏",
    "–Ω–∞—à—ë–ø—Ç—ã–≤–∞–µ—Ç",
    "–Ω–∞—à–µ–ø—Ç—ã–≤–∞–ª–∏",
    "–ª–∞—Å–∫–∞–µ—Ç",
    "–ª–∞—Å–∫–∞—é—Ç",
    "–º–∞–Ω–∏—Ç",
    "–º–∞–Ω—è—Ç",
    "–º–∞–Ω—è—â–∏–π",
    "–º–∞–Ω—è—â–∞—è",
    "–º–∞–Ω—è—â–µ–µ",
    "–º–µ—Å—Ç–æ —Å–∏–ª—ã",
    "–º–∞–≥–∏—è –º–µ—Å—Ç–∞",
    "–æ–∫—É—Ç—ã–≤–∞–µ—Ç",
    "–æ–∫—É—Ç—ã–≤–∞—é—Ç",
    "—É–∫—É—Ç—ã–≤–∞–µ—Ç",
    "—É–∫—É—Ç—ã–≤–∞—é—Ç",
    "–æ–±–Ω–∏–º–∞–µ—Ç",
    "–æ–±–Ω–∏–º–∞—é—Ç",
    "–¥–∞—Ä–∏—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ",
    "–¥–∞—Ä–∏—Ç –æ—â—É—â–µ–Ω–∏–µ",
    "–¥–∞—Ä–∏—Ç —Ç–µ–ø–ª–æ",
    "—Å–æ–∑–¥–∞—ë—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ",
    "—Å–æ–∑–¥–∞—ë—Ç –æ—Å–æ–±—É—é –∞—Ç–º–æ—Å—Ñ–µ—Ä—É",
    "–æ—Å–æ–±–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞",
    "–Ω–µ–ø–æ–≤—Ç–æ—Ä–∏–º–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞",
    "—Å–∫–∞–∑–æ—á–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞",
    "–≤–æ–ª—à–µ–±–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞",
    "–¥–µ–ª–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ",
    "–Ω–∞–ø–æ–ª–Ω—è–µ—Ç —Ç–µ–ø–ª–æ–º",
    "–Ω–∞–ø–æ–ª–Ω—è–µ—Ç —ç–Ω–µ—Ä–≥–∏–µ–π",
    "–∑–∞—Ä—è–∂–∞–µ—Ç —ç–Ω–µ—Ä–≥–∏–µ–π",
    "–∑–∞—Ä—è–∂–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º",
    "—É—é—Ç–Ω—ã–π —É–≥–æ–ª–æ–∫",
    "—É—é—Ç–Ω—ã–π —É–≥–æ–ª–æ—á–µ–∫",
    "—É—é—Ç–Ω–æ–µ –º–µ—Å—Ç–µ—á–∫–æ",
    "—Å–ª–æ–≤–Ω–æ –∏–∑ –æ—Ç–∫—Ä—ã—Ç–∫–∏",
    "–∫–∞–∫ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç–∫–µ",
    "—Å–ª–æ–≤–Ω–æ –∫–∞–¥—Ä –∏–∑ —Ñ–∏–ª—å–º–∞",
    "–∫–∞–∫ –∫–∞–¥—Ä –∏–∑ —Ñ–∏–ª—å–º–∞",
    "–∫–∞–∫ –∏–∑ —Ñ–∏–ª—å–º–∞",
    "–∫–∞–∫ –≤ –∫–∏–Ω–æ",
    "–∫–∞–∫ –∏–∑ —Å–∫–∞–∑–∫–∏",
    "—Å–ª–æ–≤–Ω–æ –≤ —Å–∫–∞–∑–∫–µ",
    "–≤–æ–ª—à–µ–±–Ω—ã–π",
    "–≤–æ–ª—à–µ–±–Ω–∞—è",
    "–≤–æ–ª—à–µ–±–Ω–æ–µ",
    "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å—Ç–æ–∏—Ç –ø–æ–±—ã–≤–∞—Ç—å",
    "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–≥–ª—è–Ω–∏—Ç–µ",
    "–∫–∞–∂–¥—ã–π –Ω–∞–π–¥—ë—Ç —á—Ç–æ-—Ç–æ —Å–≤–æ—ë",
    "–∑–¥–µ—Å—å –Ω–∞–π–¥—ë—Ç —á—Ç–æ-—Ç–æ —Å–≤–æ—ë –∫–∞–∂–¥—ã–π",
)
POSTCARD_MARINE_KEYWORDS = ("–º–æ—Ä–µ", "–º–æ—Ä—Å–∫", "sea", "ocean", "coast", "shore", "beach")
POSTCARD_RUBRIC_TAG_THRESHOLD = 9
POSTCARD_BIRD_TAGS = (
    "bird",
    "birds",
    "swan",
    "swans",
    "duck",
    "ducks",
    "seagull",
    "seagulls",
    "goose",
    "geese",
    "–ª–µ–±–µ–¥—å",
    "–ª–µ–±–µ–¥–∏",
    "—É—Ç–∫–∞",
    "—É—Ç–∫–∏",
    "—á–∞–π–∫–∞",
    "—á–∞–π–∫–∏",
    "–≥—É—Å—å",
    "–≥—É—Å–∏",
    "–∂—É—Ä–∞–≤–ª—å",
    "–∂—É—Ä–∞–≤–ª–∏",
    "–∞–∏—Å—Ç",
    "–∞–∏—Å—Ç—ã",
    "–±–∞–∫–ª–∞–Ω",
    "–±–∞–∫–ª–∞–Ω—ã",
    "–ø—Ç–∏—Ü–∞",
    "–ø—Ç–∏—Ü—ã",
)
POSTCARD_BIRD_TAG_KEYS = {tag.casefold() for tag in POSTCARD_BIRD_TAGS}
_LATIN_WORD_PATTERN = re.compile(r"[A-Za-z]")
_MARKDOWN_ESCAPE_PATTERN = re.compile(r"([\\_*[\]()])")
_POSTCARD_COMMON_STOPWORDS: tuple[str, ...] | None = None
_POSTCARD_TZ = ZoneInfo("Europe/Kaliningrad")
_POSTCARD_SEASON_AGE_THRESHOLD_DAYS = 60
SeasonName = Literal["winter", "spring", "summer", "autumn"]


def _now_kaliningrad() -> datetime:
    return datetime.now(tz=_POSTCARD_TZ)


def _parse_asset_datetime(value: Any) -> datetime | None:
    if value is None:
        return None
    if isinstance(value, datetime):
        dt = value
    else:
        text = str(value).strip()
        if not text:
            return None
        normalized = text
        if normalized.upper().endswith("Z"):
            normalized = f"{normalized[:-1]}+00:00"
        match = re.match(r"(.*)([+-]\d{2})(\d{2})$", normalized)
        if match and ":" not in match.group(3):
            normalized = f"{match.group(1)}{match.group(2)}:{match.group(3)}"
        try:
            dt = datetime.fromisoformat(normalized)
        except ValueError:
            try:
                timestamp = float(text)
            except (TypeError, ValueError):
                return None
            dt = datetime.fromtimestamp(timestamp, tz=timezone.utc)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(_POSTCARD_TZ)


def _resolve_photo_datetime(asset: Asset) -> datetime | None:
    candidates = (getattr(asset, "captured_at", None), getattr(asset, "created_at", None))
    for candidate in candidates:
        dt = _parse_asset_datetime(candidate)
        if dt is not None:
            return dt
    return None


def _get_season(value: datetime | date) -> SeasonName:
    month = value.month
    if month in (3, 4, 5):
        return "spring"
    if month in (6, 7, 8):
        return "summer"
    if month in (9, 10, 11):
        return "autumn"
    return "winter"


def _format_postcard_season_label(photo_date: datetime) -> str:
    season = _get_season(photo_date)
    year = photo_date.year
    if season == "spring":
        return f"–≤–µ—Å–Ω–∞ {year}"
    if season == "summer":
        return f"–ª–µ—Ç–æ {year}"
    if season == "autumn":
        return f"–æ—Å–µ–Ω—å {year}"
    # winter
    if photo_date.month == 12:
        start_year = year
    else:
        start_year = year - 1
    return f"–∑–∏–º–∞ {start_year}/{start_year + 1}"


def _resolve_postcard_season_line(
    asset: Asset,
    *,
    photo_dt: datetime | None = None,
    today: date | None = None,
) -> str | None:
    if photo_dt is None:
        photo_dt = _resolve_photo_datetime(asset)
    return _resolve_postcard_season_line_from_datetime(photo_dt, today=today)


def _resolve_postcard_season_line_from_datetime(
    photo_dt: datetime | None,
    *,
    today: date | None = None,
) -> str | None:
    if photo_dt is None:
        return None
    today_value = today or _now_kaliningrad().date()
    age_days = (today_value - photo_dt.date()).days
    if age_days <= _POSTCARD_SEASON_AGE_THRESHOLD_DAYS:
        return None
    return _format_postcard_season_label(photo_dt)


def _is_postcard_out_of_season(
    photo_dt: datetime | None,
    *,
    reference: datetime | None = None,
) -> bool:
    if photo_dt is None:
        return False
    now_value = reference or _now_kaliningrad()
    return _get_season(photo_dt) != _get_season(now_value)


def _append_season_line(text: str, season_line: str | None) -> str:
    cleaned = text.strip()
    if not season_line:
        return cleaned
    if cleaned:
        return f"{cleaned}\n\n{season_line}"
    return season_line


def _append_map_links(text: str, map_line: str | None) -> str:
    cleaned_text = text.strip()
    cleaned_line = (map_line or "").strip()
    if not cleaned_line:
        return cleaned_text
    if cleaned_text:
        return f"{cleaned_text}\n\n{cleaned_line}"
    return cleaned_line


def _attach_link_block(text: str, link_block: str) -> str:
    cleaned_text = text.strip()
    cleaned_link = (link_block or "").strip()
    if not cleaned_link:
        return cleaned_text
    if cleaned_text:
        return f"{cleaned_text}\n\n{cleaned_link}"
    return cleaned_link


def _sanitize_prompt_leaks(text: str) -> str:
    from main import sanitize_prompt_leaks  # avoid circular import at module load

    return sanitize_prompt_leaks(text)


def _build_link_block() -> str:
    from main import build_rubric_link_block  # avoid circular import at module load

    return build_rubric_link_block("postcard", parse_mode="Markdown")


def _build_postcard_map_links(asset: Asset) -> str | None:
    latitude = asset.latitude
    longitude = asset.longitude
    if latitude is None or longitude is None:
        return None
    return (
        "üìç "
        f"[2–ì–ò–°](https://2gis.ru/?m={longitude:.6f},{latitude:.6f})  "
        f"[–Ø–Ω–¥–µ–∫—Å](https://yandex.ru/maps/?ll={longitude:.6f},{latitude:.6f}&z=15)"
    )


def _normalize_hashtag_candidate(value: str | None) -> str | None:
    if not value:
        return None
    text = str(value).strip()
    if not text:
        return None
    text = text.replace(" ", "")
    if not text:
        return None
    if not text.startswith("#"):
        text = f"#{text.lstrip('#')}"
    return text


def _normalize_city_hashtag(value: str | None) -> str | None:
    if not value:
        return None
    text = unicodedata.normalize("NFKC", str(value)).strip()
    if not text:
        return None
    text = text.replace("—ë", "–µ").replace("–Å", "–ï")
    cleaned = re.sub(r"[^\w]+", "", text, flags=re.UNICODE)
    if not cleaned:
        return None
    return f"#{cleaned}"


def _deduplicate_hashtags(tags: Iterable[str]) -> list[str]:
    seen: set[str] = set()
    result: list[str] = []
    for tag in tags:
        normalized = _normalize_hashtag_candidate(tag)
        if not normalized:
            continue
        key = normalized.lstrip("#").lower()
        if key in seen:
            continue
        seen.add(key)
        result.append(normalized)
    return result


def _get_postcard_common_stopwords() -> tuple[str, ...]:
    global _POSTCARD_COMMON_STOPWORDS
    if _POSTCARD_COMMON_STOPWORDS is not None:
        return _POSTCARD_COMMON_STOPWORDS
    try:
        from flowers_patterns import load_flowers_knowledge

        kb = load_flowers_knowledge()
        words = sorted(kb.banned_words or [])
        _POSTCARD_COMMON_STOPWORDS = tuple(words)
    except Exception:
        logging.debug("POSTCARD_CAPTION common_stopwords_unavailable", exc_info=True)
        _POSTCARD_COMMON_STOPWORDS = ()
    return _POSTCARD_COMMON_STOPWORDS


def _collect_postcard_stopwords(stopwords: Sequence[str]) -> list[str]:
    pool: list[str] = []
    sources: tuple[Sequence[str] | tuple[str, ...], ...] = (
        stopwords,
        _get_postcard_common_stopwords(),
        POSTCARD_ADDITIONAL_STOP_PHRASES,
    )
    for source in sources:
        for item in source:
            text = str(item or "").strip()
            if text:
                pool.append(text)
    dedup: dict[str, str] = {}
    for word in pool:
        key = word.casefold()
        if key and key not in dedup:
            dedup[key] = word
    return list(dedup.values())


def _contains_banned_word(text: str, banned_words: Iterable[str]) -> bool:
    if not banned_words:
        return False
    lowered = text.casefold()
    tokens = {token.strip() for token in re.split(r"\W+", lowered) if token.strip()}
    for word in banned_words:
        normalized = str(word or "").strip().casefold()
        if not normalized:
            continue
        if normalized in tokens:
            return True
        pattern = re.escape(normalized)
        if re.search(rf"(?<!\w){pattern}(?!\w)", lowered):
            return True
    return False


def _build_postcard_opening(location: _LocationInfo) -> str:
    label = (location.city or location.region or "").strip()
    if label:
        opening = f"–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –∫—Ä–∞—Å–∏–≤—ã–º –≤–∏–¥–æ–º {label}."
        return _sanitize_sentence(opening)
    return POSTCARD_OPENING_CHOICES[0]


def _sanitize_sentence(text: str) -> str:
    if not text:
        return ""
    cleaned = _sanitize_prompt_leaks(str(text).strip())
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def _ensure_sentence_punctuation(value: str) -> str:
    if not value:
        return ""
    if value[-1] not in ".!?‚Ä¶":
        return f"{value}."
    return value


def _sanitize_postcard_caption_text(text: str) -> str:
    if not text:
        return ""
    cleaned = _sanitize_prompt_leaks(str(text).strip())
    cleaned = re.sub(r"[ \t]+\n", "\n", cleaned)
    cleaned = re.sub(r"\n{2,}", "\n", cleaned)
    cleaned = re.sub(r"\s{2,}", " ", cleaned)
    return cleaned.strip()


def _escape_markdown_text(text: str) -> str:
    if not text:
        return ""
    return _MARKDOWN_ESCAPE_PATTERN.sub(r"\\\1", text)


def _remove_latin_words(text: str) -> str:
    if not text:
        return ""
    parts = re.split(r"(\s+)", text)
    filtered: list[str] = []
    for part in parts:
        if not part:
            continue
        if part.isspace() or part.startswith("#"):
            filtered.append(part)
            continue
        if _LATIN_WORD_PATTERN.search(part):
            continue
        filtered.append(part)
    rebuilt = "".join(filtered)
    rebuilt = re.sub(r"\s{2,}", " ", rebuilt)
    return rebuilt.strip()


def _location_value_in_text(text: str, candidate: str) -> bool:
    normalized_text = text.casefold()
    normalized_candidate = candidate.casefold().strip()
    if not normalized_candidate:
        return False
    if normalized_candidate in normalized_text:
        return True
    tokens = [token for token in re.split(r"[\s-]+", normalized_candidate) if token]
    for token in tokens:
        if len(token) < 4:
            continue
        if token in normalized_text:
            return True
        if len(token) > 5 and token[:-1] in normalized_text:
            return True
        if len(token) > 6 and token[:-2] in normalized_text:
            return True
    return False


def _mentions_location(text: str, location: _LocationInfo) -> bool:
    normalized_text = text.casefold()
    candidates = [location.city, location.region, location.display]
    for candidate in candidates:
        if candidate and _location_value_in_text(normalized_text, candidate):
            return True
    return False


def _looks_like_marine_tag(tag: str) -> bool:
    normalized = str(tag or "").strip()
    if not normalized:
        return False
    key = normalized.lstrip("#").casefold()
    return any(keyword in key for keyword in POSTCARD_MARINE_KEYWORDS)


def _is_sea_scene(asset: Asset) -> bool:
    results = asset.vision_results or {}
    if isinstance(results, dict):
        if bool(results.get("is_sea")):
            return True
        sea_wave = results.get("sea_wave_score")
        value: float | None = None
        if isinstance(sea_wave, dict):
            value = sea_wave.get("value")  # type: ignore[assignment]
        else:
            value = sea_wave  # type: ignore[assignment]
        try:
            if value is not None and float(value) >= 0:
                return True
        except (TypeError, ValueError):
            pass

    return False


@dataclass(slots=True)
class _LocationInfo:
    display: str
    city: str | None
    region: str | None
    country: str | None


def _resolve_location(asset: Asset) -> _LocationInfo:
    city = (asset.city or "").strip() or None
    region = getattr(asset, "region", None)
    if isinstance(region, str):
        region = region.strip() or None
    country = (asset.country or "").strip() or None
    display = city or region or country or "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å"
    return _LocationInfo(display=display, city=city, region=region, country=country)


def _collect_semantic_tags(asset: Asset) -> list[str]:
    tags: list[str] = []
    results = asset.vision_results or {}
    raw_tags = results.get("tags") if isinstance(results, dict) else None
    if isinstance(raw_tags, (list, tuple, set)):
        for tag in raw_tags:
            text = str(tag or "").strip()
            if text and text not in tags:
                tags.append(text)
    if isinstance(results, dict):
        if results.get("is_sunset") and "–∑–∞–∫–∞—Ç" not in tags:
            tags.append("–∑–∞–∫–∞—Ç")
        if results.get("photo_weather_display"):
            weather = str(results.get("photo_weather_display") or "").strip()
            if weather and weather not in tags:
                tags.append(weather)
    return tags[:8]


def _collect_bird_tags(asset: Asset) -> list[str]:
    results = asset.vision_results or {}
    raw_tags = results.get("tags") if isinstance(results, dict) else None
    if not isinstance(raw_tags, (list, tuple, set)):
        return []
    found: list[str] = []
    seen: set[str] = set()
    for tag in raw_tags:
        text = str(tag or "").strip()
        if not text:
            continue
        key = text.casefold()
        if key in POSTCARD_BIRD_TAG_KEYS and key not in seen:
            seen.add(key)
            found.append(text)
    return found


def _finalize_postcard_hashtags(
    candidate_tags: Iterable[str],
    region_hashtag: str | None,
    city_hashtag: str | None,
    *,
    is_sea_scene: bool,
    include_rubric_tag: bool,
    fallback_keywords: Sequence[str] | None = None,
) -> list[str]:
    prepared = _deduplicate_hashtags(candidate_tags)
    filtered: list[str] = []
    rubric_key = POSTCARD_RUBRIC_HASHTAG.lstrip("#").casefold()
    for tag in prepared:
        if not tag:
            continue
        key = tag.lstrip("#").casefold()
        if key in POSTCARD_BANNED_TAG_KEYS:
            continue
        if key == rubric_key and not include_rubric_tag:
            continue
        if not is_sea_scene and _looks_like_marine_tag(tag):
            continue
        filtered.append(tag)
    normalized_region_tag = _normalize_hashtag_candidate(region_hashtag)
    normalized_city_tag = _normalize_hashtag_candidate(city_hashtag)
    region_value = normalized_region_tag or (
        region_hashtag.strip() if isinstance(region_hashtag, str) else None
    )
    city_value = normalized_city_tag or (
        city_hashtag.strip() if isinstance(city_hashtag, str) else None
    )
    region_required_key: str | None = None
    if region_value:
        filtered.append(region_value)
        region_required_key = region_value.casefold()
    if city_value:
        filtered.append(city_value)
    if include_rubric_tag:
        filtered.append(POSTCARD_RUBRIC_HASHTAG)
    combined = _deduplicate_hashtags(filtered)
    fallback_candidates: list[str] = []
    if fallback_keywords:
        for keyword in fallback_keywords:
            candidate = _normalize_hashtag_candidate(keyword)
            if not candidate:
                continue
            key = candidate.lstrip("#").casefold()
            if key in POSTCARD_BANNED_TAG_KEYS:
                continue
            if key == rubric_key and not include_rubric_tag:
                continue
            if not is_sea_scene and _looks_like_marine_tag(candidate):
                continue
            if candidate not in combined:
                fallback_candidates.append(candidate)
    while len(combined) < POSTCARD_MIN_HASHTAGS and fallback_candidates:
        combined.append(fallback_candidates.pop(0))
    if len(combined) < POSTCARD_MIN_HASHTAGS:
        for fallback in POSTCARD_DEFAULT_HASHTAGS:
            candidate = _normalize_hashtag_candidate(fallback) or fallback
            key = candidate.lstrip("#").casefold()
            if key in POSTCARD_BANNED_TAG_KEYS:
                continue
            if key == rubric_key and not include_rubric_tag:
                continue
            if candidate not in combined:
                combined.append(candidate)
            if len(combined) >= POSTCARD_MIN_HASHTAGS:
                break
    required_keys: set[str] = set()
    if region_required_key:
        required_keys.add(region_required_key)
    if include_rubric_tag:
        required_keys.add(POSTCARD_RUBRIC_HASHTAG.casefold())
    return _limit_postcard_hashtags(combined, required_keys)


def _limit_postcard_hashtags(tags: list[str], required_keys: set[str]) -> list[str]:
    if len(tags) <= POSTCARD_HASHTAG_LIMIT:
        return tags
    limited: list[str] = []
    optional_indices: list[int] = []
    present_required: set[str] = set()
    for tag in tags:
        key = tag.casefold()
        is_required = key in required_keys
        if len(limited) < POSTCARD_HASHTAG_LIMIT:
            limited.append(tag)
            if is_required:
                present_required.add(key)
            else:
                optional_indices.append(len(limited) - 1)
            continue
        if is_required and key not in present_required and optional_indices:
            replace_index = optional_indices.pop()
            limited[replace_index] = tag
            present_required.add(key)
    return limited


def _postcard_fallback_sentence(location: _LocationInfo, semantic_tags: Sequence[str]) -> str:
    detail = None
    for tag in semantic_tags:
        text = str(tag or "").strip()
        if text:
            detail = text
            break
    label = location.display or "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å"
    fragment: str
    lowered = detail.casefold() if isinstance(detail, str) else ""
    if lowered and "–∑–∞–∫–∞—Ç" in lowered:
        fragment = "–∑–∞–∫–∞—Ç –º—è–≥–∫–æ –ø–æ–¥—Å–≤–µ—á–∏–≤–∞–µ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç"
    elif lowered and "–º–æ—Ä–µ" in lowered:
        fragment = "—Ç–∏—Ö–∞—è –≤–æ–¥–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ—Ä"
    elif lowered and "–≥–æ—Ä–æ–¥" in lowered:
        fragment = "–≥–æ—Ä–æ–¥—Å–∫–∏–µ –ª–∏–Ω–∏–∏ –∑–≤—É—á–∞—Ç —Å–ø–æ–∫–æ–π–Ω–æ"
    elif detail:
        fragment = f"–≤–∏–¥ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ {detail}"
    else:
        fragment = "—Å–ø–æ–∫–æ–π–Ω—ã–π –≤–∏–¥ –¥–ª—è –ø—Ä–æ–≥—É–ª–∫–∏"
    sentence = f"–≠—Ç–æ {label} ‚Äî {fragment}."
    return _sanitize_sentence(sentence)


async def generate_postcard_caption(
    openai: OpenAIClient | None,
    asset: Asset,
    region_hashtag: str,
    stopwords: Sequence[str],
) -> tuple[str, list[str]]:
    location = _resolve_location(asset)
    semantic_tags = _collect_semantic_tags(asset)
    bird_tags = _collect_bird_tags(asset)
    has_birds = bool(bird_tags)
    is_sea_scene = _is_sea_scene(asset)
    photo_datetime = _resolve_photo_datetime(asset)
    now_local = _now_kaliningrad()
    season_line = _resolve_postcard_season_line(
        asset,
        photo_dt=photo_datetime,
        today=now_local.date(),
    )
    is_out_of_season = _is_postcard_out_of_season(photo_datetime, reference=now_local)
    region_tag = _normalize_hashtag_candidate(region_hashtag)
    city_tag = _normalize_city_hashtag(location.city)
    banned_words = _collect_postcard_stopwords(stopwords)
    score_value = asset.postcard_score if isinstance(asset.postcard_score, int) else None
    include_rubric_tag = bool(
        score_value is not None and score_value >= POSTCARD_RUBRIC_TAG_THRESHOLD
    )
    context_payload: dict[str, Any] = {
        "location": location.display,
        "city": location.city,
        "region": location.region,
        "country": location.country,
        "tags": semantic_tags,
        "postcard_score": score_value,
        "location_case_hint": "prepositional",
        "has_birds": has_birds,
        "is_sea_scene": is_sea_scene,
        "is_out_of_season": is_out_of_season,
    }
    extra_hint = asset.vision_results or {}
    if isinstance(extra_hint, dict):
        if extra_hint.get("weather_final_display"):
            context_payload["weather"] = extra_hint.get("weather_final_display")
        if extra_hint.get("is_sunset"):
            context_payload["is_sunset"] = True
    if bird_tags:
        context_payload["bird_tags"] = bird_tags
    payload_text = json.dumps(context_payload, ensure_ascii=False, separators=(",", ": "))
    stopwords_text = ", ".join(banned_words) if banned_words else ""
    region_prompt = region_tag or _normalize_hashtag_candidate(region_hashtag)
    if not region_prompt and isinstance(region_hashtag, str):
        region_prompt = region_hashtag.strip()
    if not region_prompt:
        region_prompt = "#–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è–û–±–ª–∞—Å—Ç—å"
    system_prompt_lines = [
        "–¢—ã ‚Äî –≥–æ–ª–æ—Å –ø—Ä–æ–µ–∫—Ç–∞ ¬´–ö–æ—Ç–æ–ø–æ–≥–æ–¥–∞¬ª –∏ –≥–æ—Ç–æ–≤–∏—à—å –ø–æ–¥–ø–∏—Å—å –¥–ª—è —Ä—É–±—Ä–∏–∫–∏ ¬´–û—Ç–∫—Ä—ã—Ç–æ—á–Ω—ã–π –≤–∏–¥¬ª.",
        "–°–æ–±–µ—Ä–∏ –ø–æ–¥–ø–∏—Å—å –∏–∑ 2‚Äì3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–¥–æ 250 —Å–∏–º–≤–æ–ª–æ–≤) –≤ —Å–ø–æ–∫–æ–π–Ω–æ–º, —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º —Ç–æ–Ω–µ.",
        "–ü–µ—Ä–≤–∞—è —Ñ—Ä–∞–∑–∞ —Å—Ç—Ä–æ–≥–æ –æ–¥–Ω–∞ –∏–∑ –¥–≤—É—Ö: ¬´–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –æ—Ç–∫—Ä—ã—Ç–æ—á–Ω—ã–º –≤–∏–¥–æ–º.¬ª –∏–ª–∏ ¬´–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –∫—Ä–∞—Å–∏–≤—ã–º –≤–∏–¥–æ–º <–ª–æ–∫–∞—Ü–∏–∏>.¬ª.",
        "–ï—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –≥–æ—Ä–æ–¥ –∏–ª–∏ —Ä–µ–≥–∏–æ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π –≤—Ç–æ—Ä—É—é —Ñ–æ—Ä–º—É–ª—É –∏ –ø–æ—Å—Ç–∞–≤—å –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –ø–æ–¥—Ö–æ–¥—è—â–µ–º –ø–∞–¥–µ–∂–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–≤ –ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏¬ª, ¬´–≤ –ó–µ–ª–µ–Ω–æ–≥—Ä–∞–¥—Å–∫–µ¬ª).",
        "–ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —Ñ—Ä–∞–∑—ã –¥–æ–±–∞–≤—å 1‚Äì2 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏ —Å—Ü–µ–Ω—ã –±–µ–∑ –ø–∞—Ñ–æ—Å–∞ –∏ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞.",
        "–ü–∏—à–∏ —Ç–æ–ª—å–∫–æ –ø–æ-—Ä—É—Å—Å–∫–∏, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞, —ç–º–æ–¥–∑–∏, —Ö—ç—à—Ç–µ–≥–∏ –∏–ª–∏ —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞.",
        "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞–∑–æ–≤–∏ —É–∫–∞–∑–∞–Ω–Ω—É—é –ª–æ–∫–∞—Ü–∏—é (–≥–æ—Ä–æ–¥ –∏–ª–∏ —Ä–µ–≥–∏–æ–Ω) –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ñ–æ—Ä–º–µ.",
        "–°–æ—Ö—Ä–∞–Ω—è–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π —Ç–æ–Ω, –∏–∑–±–µ–≥–∞–π —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —à—Ç–∞–º–ø–æ–≤.",
    ]
    if is_out_of_season:
        system_prompt_lines.append(
            "–ö–æ–≥–¥–∞ is_out_of_season=true, —Ñ–æ—Ç–æ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Å–µ–∑–æ–Ω–æ–º —Å–µ–π—á–∞—Å, –ø–æ—ç—Ç–æ–º—É –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Ñ—Ä–∞–∑—ã –æ–ø–∏—à–∏ —Å—Ü–µ–Ω—É –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—à–µ–¥—à–µ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ª—ë–≥–∫–∏–º –Ω–æ—Å—Ç–∞–ª—å–≥–∏—á–µ—Å–∫–∏–º –æ—Ç—Ç–µ–Ω–∫–æ–º –∏ –±–µ–∑ –∫—Ä–∏–Ω–∂-–Ω–æ—Å—Ç–∞–ª—å–≥–∏–∏."
        )
        system_prompt_lines.append(
            "–ú–æ–∂–µ—à—å –Ω–∞—á–∞—Ç—å –≤—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –º—è–≥–∫–∏–º–∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –≤—Ä–æ–¥–µ ¬´–í—Å–ø–æ–º–Ω–∏–º‚Ä¶¬ª, ¬´–í—Å–ø–æ–º–Ω–∏‚Ä¶¬ª, ¬´–ü–æ–º–Ω–∏—Ç–µ, –∫–∞–∫‚Ä¶¬ª –∏–ª–∏ ¬´–ù–∞–ø–æ–º–Ω—é –≤–∞–º‚Ä¶¬ª, –µ—Å–ª–∏ —ç—Ç–æ –∑–≤—É—á–∏—Ç —Ç–µ–ø–ª–æ –∏ –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏."
        )
    else:
        system_prompt_lines.append(
            "–ö–æ–≥–¥–∞ is_out_of_season=false, –æ–ø–∏—Å—ã–≤–∞–π –¥–µ—Ç–∞–ª–∏ –≤ –Ω–∞—Å—Ç–æ—è—â–µ–º –≤—Ä–µ–º–µ–Ω–∏, –±—É–¥—Ç–æ —Å—Ü–µ–Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å."
        )
    if banned_words:
        system_prompt_lines.append("–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —à—Ç–∞–º–ø—ã –∏ –∑–∞–µ–∑–∂–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã.")
        system_prompt_lines.append(
            "–ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞ –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ (–∏ –ª—é–±—ã–µ –∏—Ö —Ñ–æ—Ä–º—ã): "
            f"{stopwords_text}."
        )
        system_prompt_lines.append(
            "–ï—Å–ª–∏ —Ö–æ—á–µ—Ç—Å—è –ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ—Ö–æ–∂–∏–π —Å–º—ã—Å–ª ‚Äî –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏ –ø—Ä–æ—Å—Ç—ã–º, –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º."
        )
    system_prompt_lines.append(
        "–ï—Å–ª–∏ has_birds=true –∏–ª–∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã bird_tags, –º—è–≥–∫–æ —É–ø–æ–º—è–Ω–∏ –∑–∞–º–µ—Ç–Ω—ã—Ö –ø—Ç–∏—Ü (–ª–µ–±–µ–¥–∏, —É—Ç–∫–∏, —á–∞–π–∫–∏ –∏ —Ç. –ø.), –µ—Å–ª–∏ —ç—Ç–æ –æ—Ä–≥–∞–Ω–∏—á–Ω–æ."
    )
    system_prompt_lines.append(
        f"–•—ç—à—Ç–µ–≥–∏ –≤–æ–∑–≤—Ä–∞—â–∞–π –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–∞—Å—Å–∏–≤–æ–º: –≤—Å–µ–≥–æ 3‚Äì5 —Ç–µ–≥–æ–≤, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∞–π —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ–≥ {region_prompt} –∏ 2‚Äì4 —Å–º—ã—Å–ª–æ–≤—ã—Ö."
    )
    if city_tag:
        system_prompt_lines.append(
            "–ï—Å–ª–∏ –µ—Å—Ç—å city, –¥–æ–±–∞–≤—å –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ö—ç—à—Ç–µ–≥ —Å –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ #–ì–æ—Ä–æ–¥."
        )
    system_prompt_lines.append("–•—ç—à—Ç–µ–≥–∏ #–∫–æ—Ç–æ–ø–æ–≥–æ–¥–∞ –∏ –¥—É–±–ª–∏ –∑–∞–ø—Ä–µ—â–µ–Ω—ã.")
    if include_rubric_tag:
        system_prompt_lines.append(
            f"–ü—Ä–∏ –≤—ã—Å–æ–∫–æ–º postcard_score –¥–æ–±–∞–≤—å —Ç–µ–≥ {POSTCARD_RUBRIC_HASHTAG} (—Ç–æ–ª—å–∫–æ –≤ –º–∞—Å—Å–∏–≤–µ hashtags)."
        )
    else:
        system_prompt_lines.append(
            f"–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ–≥ {POSTCARD_RUBRIC_HASHTAG} –ø—Ä–∏ —Ç–µ–∫—É—â–µ–º postcard_score."
        )
    system_prompt_lines.append("–°—Å—ã–ª–∫—É ¬´–ü–æ–ª—é–±–∏—Ç—å 39¬ª –¥–æ–±–∞–≤–∏—Ç —Å–∏—Å—Ç–µ–º–∞ ‚Äî –Ω–µ —É–ø–æ–º–∏–Ω–∞–π –µ—ë –≤ —Ç–µ–∫—Å—Ç–µ.")
    system_prompt = "\n".join(system_prompt_lines)
    bird_info_lines = [f"has_birds: {'true' if has_birds else 'false'}."]
    if bird_tags:
        bird_info_lines.append(f"bird_tags: {', '.join(bird_tags)}.")
    bird_info = " ".join(bird_info_lines)
    user_prompt = (
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ü–µ–Ω—ã (JSON):\n"
        f"{payload_text}\n\n"
        '–°—Ñ–æ—Ä–º–∏—Ä—É–π JSON {"caption":"...","hashtags":["#..."]}.\n'
        "Caption ‚Äî –ø–æ–ª–Ω–∞—è –ø–æ–¥–ø–∏—Å—å –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º system prompt (–±–µ–∑ —Ö—ç—à—Ç–µ–≥–æ–≤, —Å—Å—ã–ª–æ–∫ –∏ ¬´–ü–æ–ª—é–±–∏—Ç—å 39¬ª).\n"
        "Hashtags ‚Äî 3‚Äì5 —Ç–µ–≥–æ–≤ —Å —Å–∏–º–≤–æ–ª–æ–º #, –±–µ–∑ #–∫–æ—Ç–æ–ø–æ–≥–æ–¥–∞ –∏ –ø–æ–≤—Ç–æ—Ä–æ–≤. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ–≥ "
        f"{region_prompt} –∏ –ø–æ–¥–±–µ—Ä–∏ 2‚Äì4 —Å–º—ã—Å–ª–æ–≤—ã—Ö —Ç–µ–≥–∞ –ø–æ —Å—Ü–µ–Ω–µ.\n"
    )
    if city_tag:
        user_prompt += "–ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω city, –¥–æ–±–∞–≤—å –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–µ–≥ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –≥–æ—Ä–æ–¥–∞ (#–ì–æ—Ä–æ–¥).\n"
    if include_rubric_tag:
        user_prompt += f"–î–æ–±–∞–≤—å —Ç–µ–≥ {POSTCARD_RUBRIC_HASHTAG} –≤ –º–∞—Å—Å–∏–≤ hashtags.\n"
    else:
        user_prompt += f"–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ–≥ {POSTCARD_RUBRIC_HASHTAG}.\n"
    user_prompt += f"\n–ü–æ–¥—Å–∫–∞–∑–∫–∞ –æ –ø—Ç–∏—Ü–∞—Ö:\n{bird_info}"
    schema = {
        "type": "object",
        "properties": {
            "caption": {"type": "string"},
            "hashtags": {
                "type": "array",
                "items": {"type": "string"},
                "minItems": 3,
                "maxItems": 5,
            },
        },
        "required": ["caption", "hashtags"],
    }
    region_value_for_tags = region_prompt
    default_tags = _finalize_postcard_hashtags(
        [],
        region_value_for_tags,
        city_tag,
        is_sea_scene=is_sea_scene,
        include_rubric_tag=include_rubric_tag,
        fallback_keywords=semantic_tags,
    )
    map_links_line = _build_postcard_map_links(asset)
    link_block = _build_link_block()
    if not openai or not getattr(openai, "api_key", None):
        fallback_sentence = _postcard_fallback_sentence(location, semantic_tags)
        opening = _build_postcard_opening(location)
        combined = _remove_latin_words(f"{opening} {fallback_sentence}".strip())
        combined = _escape_markdown_text(combined)
        caption_with_map = _append_map_links(combined, map_links_line)
        caption_body = _append_season_line(caption_with_map, season_line)
        caption_with_block = _attach_link_block(caption_body, link_block)
        return caption_with_block.strip(), default_tags
    attempts = 3
    for attempt in range(1, attempts + 1):
        try:
            logging.info(
                "POSTCARD_CAPTION request model=%s attempt=%s/%s", "gpt-4o", attempt, attempts
            )
            response = await openai.generate_json(
                model="gpt-4o",
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                schema=schema,
                temperature=0.95,
                top_p=0.9,
            )
        except Exception:
            logging.exception("POSTCARD_CAPTION openai_error attempt=%s", attempt)
            continue
        if not response or not isinstance(response.content, dict):
            logging.warning("POSTCARD_CAPTION invalid_response attempt=%s", attempt)
            continue
        hashtag_raw = response.content.get("hashtags")
        if not isinstance(hashtag_raw, list):
            hashtag_raw = []
        caption_raw = response.content.get("caption")
        if not isinstance(caption_raw, str):
            caption_raw = response.content.get("sentence")
        if not isinstance(caption_raw, str):
            logging.warning("POSTCARD_CAPTION missing_caption attempt=%s", attempt)
            continue
        caption_text = _sanitize_postcard_caption_text(caption_raw)
        if not caption_text:
            logging.info("POSTCARD_CAPTION empty_caption attempt=%s", attempt)
            continue
        caption_text = re.sub(r"#\S+", "", caption_text).strip()
        caption_text = _remove_latin_words(caption_text)
        caption_text = re.sub(r"\s{2,}", " ", caption_text).strip()
        if not caption_text:
            logging.info("POSTCARD_CAPTION empty_after_latin attempt=%s", attempt)
            continue
        if caption_text[-1] not in ".!?‚Ä¶":
            caption_text = f"{caption_text}."
        opening_sentence = re.split(r"(?<=[.!?‚Ä¶])\s+", caption_text)[0]
        lowered_opening = opening_sentence.casefold()
        if not any(
            lowered_opening.startswith(prefix.casefold()) for prefix in POSTCARD_OPENING_CHOICES
        ):
            logging.info("POSTCARD_CAPTION invalid_opening attempt=%s", attempt)
            continue
        if banned_words and _contains_banned_word(caption_text, banned_words):
            logging.info("POSTCARD_CAPTION banned_word attempt=%s", attempt)
            continue
        if (location.city or location.region) and not _mentions_location(caption_text, location):
            logging.info("POSTCARD_CAPTION missing_location attempt=%s", attempt)
            continue
        hashtags = _finalize_postcard_hashtags(
            hashtag_raw,
            region_value_for_tags,
            city_tag,
            is_sea_scene=is_sea_scene,
            include_rubric_tag=include_rubric_tag,
            fallback_keywords=semantic_tags,
        )
        escaped_caption = _escape_markdown_text(caption_text)
        caption_with_map = _append_map_links(escaped_caption, map_links_line)
        caption_with_season = _append_season_line(caption_with_map, season_line)
        caption_with_block = _attach_link_block(caption_with_season, link_block)
        return caption_with_block.strip(), hashtags
    logging.warning("POSTCARD_CAPTION fallback_used")
    fallback_sentence = _postcard_fallback_sentence(location, semantic_tags)
    opening = _build_postcard_opening(location)
    combined = _remove_latin_words(f"{opening} {fallback_sentence}".strip())
    combined = _escape_markdown_text(combined)
    caption_with_map = _append_map_links(combined, map_links_line)
    caption_body = _append_season_line(caption_with_map, season_line)
    caption_with_block = _attach_link_block(caption_body, link_block)
    return caption_with_block.strip(), default_tags


async def generate_sea_caption(
    bot: Bot,
    *,
    storm_state: str,
    storm_persisting: bool,
    wave_height_m: float | None,
    wave_score: float,
    wind_class: str | None,
    wind_ms: float | None,
    wind_kmh: float | None,
    clouds_label: str,
    sunset_selected: bool,
    want_sunset: bool,
    place_hashtag: str | None,
    fact_sentence: str | None,
    now_local_iso: str | None = None,
    day_part: str | None = None,
    tz_name: str | None = None,
    job: Job | None = None,
) -> tuple[str, list[str]]:
    default_hashtags = bot._default_hashtags("sea")
    leads = [
        "–ê –≤—ã –∑–Ω–∞–ª–∏",
        "–ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã",
        "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç:",
        "–≠—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:",
        "–ö—Å—Ç–∞—Ç–∏:",
        "–ö —Å–ª–æ–≤—É –æ –ë–∞–ª—Ç–∏–∫–µ,",
        "–¢–µ–ø–µ—Ä—å –≤—ã –±—É–¥–µ—Ç–µ –∑–Ω–∞—Ç—å:",
        "–ù–µ–±–æ–ª—å—à–æ–π —Ñ–∞–∫—Ç:",
        "–ü–æ–¥–µ–ª—é—Å—å —Ñ–∞–∫—Ç–æ–º:",
        "–ö —Å–ª–æ–≤—É",
    ]

    def fallback_caption() -> str:
        if storm_state == "strong_storm":
            opening = "–°–µ–≥–æ–¥–Ω—è —Å–∏–ª—å–Ω—ã–π —à—Ç–æ—Ä–º –Ω–∞ –º–æ—Ä–µ ‚Äî –≤–æ–ª–Ω—ã –≥—Ä–µ–º—è—Ç —É —Å–∞–º–æ–≥–æ –±–µ—Ä–µ–≥–∞."
        elif storm_state == "storm":
            if storm_persisting:
                opening = "–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —à—Ç–æ—Ä–º–∏—Ç—å –Ω–∞ –º–æ—Ä–µ ‚Äî –≤–æ–ª–Ω—ã –≤—Å—ë –µ—â—ë –±—å—é—Ç –æ –±–µ—Ä–µ–≥."
            else:
                opening = "–°–µ–≥–æ–¥–Ω—è —à—Ç–æ—Ä–º –Ω–∞ –º–æ—Ä–µ ‚Äî –≤–æ–ª–Ω—ã —É–ø—Ä—è–º–æ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –æ –∫—Ä–æ–º–∫—É."
        else:
            opening = (
                "–ü–æ—Ä–∞–¥—É—é –∑–∞–∫–∞—Ç–æ–º –Ω–∞–¥ –º–æ—Ä–µ–º ‚Äî –ø–æ–±–µ—Ä–µ–∂—å–µ –¥—ã—à–∏—Ç —Ç–µ–ø–ª–æ–º."
                if sunset_selected
                else "–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –º–æ—Ä–µ–º ‚Äî —Ç–∏—Ö–∏–π –±–µ—Ä–µ–≥ –∏ —Ä–æ–≤–Ω—ã–π –ø–ª–µ—Å–∫."
            )
        second_para = ""
        if fact_sentence:
            lead = random.choice(leads)
            fact_text = fact_sentence.strip()
            if lead.endswith(":") or lead.endswith(","):
                second_para = f"{lead} {fact_text}"
            else:
                second_para = f"{lead}, {fact_text}"
        if second_para:
            return f"{opening}\n\n{second_para}"
        if wind_class == "very_strong":
            return f"{opening} –í–µ—Ç–µ—Ä —Å—Ä—ã–≤–∞–µ—Ç —à–∞–ø–∫–∏ –Ω–∞ –Ω–∞–±–µ—Ä–µ–∂–Ω–æ–π."
        if wind_class == "strong":
            return f"{opening} –í–µ—Ç–µ—Ä –æ—â—É—Ç–∏–º–æ —Ç—è–Ω–µ—Ç –∫ –º–æ—Ä—é."
        if storm_state == "calm":
            return f"{opening} –ù–∞ –ø–æ–±–µ—Ä–µ–∂—å–µ —Å–ø–æ–∫–æ–π–Ω–æ –∏ —Ö–æ—á–µ—Ç—Å—è –∑–∞–¥–µ—Ä–∂–∞—Ç—å—Å—è."
        return opening

    if not bot.openai or not bot.openai.api_key:
        raw_fallback = fallback_caption()
        cleaned = bot.strip_header(raw_fallback)
        fallback_text = cleaned.strip() if cleaned else raw_fallback.strip()
        return fallback_text, default_hashtags

    day_part_instruction = ""
    if day_part:
        day_part_instruction = (
            "–£—á–∏—Ç—ã–≤–∞–π –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: now_local_iso, day_part (morning|day|evening|night), tz_name. "
            "–ü–∏—à–∏ —É–º–µ—Å—Ç–Ω–æ —Ç–µ–∫—É—â–µ–º—É –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫, –Ω–æ –Ω–µ —É–ø–æ–º–∏–Ω–∞–π –≤—Ä–µ–º—è —è–≤–Ω–æ. "
            "–ò–∑–±–µ–≥–∞–π –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π –∏ –ø–æ–∂–µ–ª–∞–Ω–∏–π, –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –º–æ–º–µ–Ω—Ç—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–ø—É—Å—Ç—å –≤–∞—à –¥–µ–Ω—å –±—É–¥–µ—Ç‚Ä¶¬ª, –µ—Å–ª–∏ —É–∂–µ –≤–µ—á–µ—Ä/–Ω–æ—á—å). "
            "–°–æ—Ö—Ä–∞–Ω—è–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω. "
        )

    system_prompt = (
        "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª–∞ –æ –ë–∞–ª—Ç–∏–π—Å–∫–æ–º –º–æ—Ä–µ.\n\n"
        "# –§–æ—Ä–º–∞ –∏ —Ç–æ–Ω\n"
        "‚Ä¢ –ü–∏—à–∏ —Ä–æ–≤–Ω–æ 2 –∞–±–∑–∞—Ü–∞: –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–µ + —Ñ–∞–∫—Ç.\n"
        "‚Ä¢ –í—Å—Ç—É–ø–ª–µ–Ω–∏–µ ‚Äî 1‚Äì2 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–ø–æ–∫–æ–π–Ω–æ –∏ –ø–æ-–¥—Ä—É–∂–µ—Å–∫–∏, –±–µ–∑ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞; –¥–æ–ø—É—Å—Ç–∏–º –æ–¥–∏–Ω —É–º–µ—Å—Ç–Ω—ã–π —ç–º–æ–¥–∑–∏.\n"
        "‚Ä¢ –§–∞–∫—Ç ‚Äî –Ω–∞ —Ç–µ–º—É –ë–∞–ª—Ç–∏–∫–∏. –ù–∞—á–Ω–∏ —Å –ø–æ–¥–≤–æ–¥–∫–∏ (–æ–¥–Ω—É –∏–∑: "
        + ", ".join(f'"{lead}"' for lead in leads)
        + ") –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ñ–∞–∫—Ç –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏—è —Å–º—ã—Å–ª–∞.\n"
        "‚Ä¢ –ü—Ä–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–π —á–∏—Å–ª–∞/–Ω–∞–∑–≤–∞–Ω–∏—è/—Ç–µ—Ä–º–∏–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏: –Ω–µ –º–µ–Ω—è–π –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã, —Ç–æ–ø–æ–Ω–∏–º—ã –∏ —Ç–µ—Ä–º–∏–Ω—ã.\n"
        "‚Ä¢ –ò–∑–±–µ–≥–∞–π —à—Ç–∞–º–ø–æ–≤ –∏ –∫–ª–∏—à–µ: ¬´–¥—ã—à–∏—Ç¬ª, ¬´—à–µ–ø—á–µ—Ç¬ª, ¬´–º–∞–Ω–∏—Ç¬ª, ¬´–ª–∞—Å–∫–∞–µ—Ç¬ª, ¬´–≤–µ–ª–∏—á–∞–≤—ã–π¬ª, ¬´–±—É—Ä–ª–∏—Ç¬ª, ¬´–æ–¥–∞—Ä—è–µ—Ç¬ª, ¬´–Ω–µ–∂–Ω—ã–π –±—Ä–∏–∑¬ª –∏ —Ç. –ø.\n"
        "‚Ä¢ –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∂–∞—Ä–≥–æ–Ω –∏ –∑–∞—É–º–Ω—ã–µ —Å–ª–æ–≤–∞ (–∏–∑–±–µ–≥–∞–π –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤): ¬´—Ç–µ—Ä–º–æ–∫–ª–∏–Ω¬ª, ¬´–≥–∞–ª–æ–∫–ª–∏–Ω¬ª, ¬´–¥–µ–Ω—Å–∏—Ç–µ—Ä–º¬ª, ¬´–±–µ–Ω—Ç–∞–ª—å¬ª, ¬´—Å–∏–Ω–æ–ø—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Å—Ç–∞–Ω–æ–≤–∫–∞¬ª, "
        "¬´—Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–æ–Ω–∞¬ª, ¬´–∏–Ω—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è¬ª, ¬´–º–æ—Ä—Ñ–æ–¥–∏–Ω–∞–º–∏–∫–∞¬ª, –∏ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º—ã.\n"
        f"‚Ä¢ {day_part_instruction}"
        "‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ê–±–∑–∞—Ü—ã —Ä–∞–∑–¥–µ–ª—è–π –æ–¥–Ω–æ–π –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.\n"
        "‚Ä¢ –ñ–Å–°–¢–ö–û–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (–±–µ–∑ —Ö—ç—à—Ç–µ–≥–æ–≤ –∏ —Å—Å—ã–ª–∫–∏) ‚Äî –Ω–µ –±–æ–ª–µ–µ 700 —Å–∏–º–≤–æ–ª–æ–≤; –ù–ò–ö–û–ì–î–ê –Ω–µ –ø—Ä–µ–≤—ã—à–∞–π 900 —Å–∏–º–≤–æ–ª–æ–≤.\n"
    )
    wind_label = wind_class if wind_class in {"strong", "very_strong"} else "none"
    prompt_payload: dict[str, Any] = {
        "storm_state": storm_state,
        "storm_persisting": storm_persisting,
        "wave_height_m": round(wave_height_m, 2) if wave_height_m is not None else None,
        "wave_score": round(wave_score, 2),
        "wind_class": wind_label,
        "wind_ms": round(wind_ms, 1) if wind_ms is not None else None,
        "wind_kmh": round(wind_kmh, 1) if wind_kmh is not None else None,
        "clouds_label": clouds_label,
        "sunset_selected": sunset_selected,
        "want_sunset": want_sunset,
        "blog_tone": True,
    }
    if place_hashtag:
        prompt_payload["place_hashtag"] = place_hashtag
    if storm_state != "strong_storm" and fact_sentence:
        prompt_payload["fact_sentence"] = fact_sentence
    if now_local_iso:
        prompt_payload["now_local_iso"] = now_local_iso
    if day_part:
        prompt_payload["day_part"] = day_part
    if tz_name:
        prompt_payload["tz_name"] = tz_name
    payload_text = json.dumps(prompt_payload, ensure_ascii=False, separators=(",", ": "))
    user_prompt = (
        "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ü–µ–Ω—ã (JSON):\n"
        f"{payload_text}\n\n"
        "–°–æ–±–µ—Ä–∏ –ø–æ–¥–ø–∏—Å—å –∫–∞–∫ –¥–≤–∞ –∞–±–∑–∞—Ü–∞.\n\n"
        "–í—Å—Ç—É–ø–ª–µ–Ω–∏–µ: 1‚Äì2 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –º–æ–∂–Ω–æ –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ (¬´–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –º–æ—Ä–µ–º¬ª). –î–æ–ø—É—Å—Ç–∏–º –æ–¥–∏–Ω —ç–º–æ–¥–∑–∏ —Ç–æ–ª—å–∫–æ –∑–¥–µ—Å—å. –ù–µ –ø–µ—Ä–µ—á–∏—Å–ª—è–π –ø–æ–≥–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —á–∏—Å–ª–∞.\n\n"
        "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç: –Ω–∞—á–Ω–∏ –æ–¥–Ω–æ–π –ø–æ–¥–≤–æ–¥–∫–æ–π –∏–∑ —Å–ø–∏—Å–∫–∞ (–∏–ª–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–π, —è–≤–Ω–æ –ø–æ–º–µ—á–∞—é—â–µ–π –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Ñ–∞–∫—Ç—É) –∏ –ø–µ—Ä–µ–¥–∞–π —Å–º—ã—Å–ª fact_sentence –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π, –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º.\n\n"
        "–õ–∏–º–∏—Ç—ã: –≤—Å—Ç—É–ø–ª–µ–Ω–∏–µ ‚â§220; –æ–±—â–∏–π ‚â§350 (–∏–ª–∏ ‚â§400, –µ—Å–ª–∏ fact_sentence –¥–ª–∏–Ω–Ω—ã–π). –ë–µ–∑ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –∏ —Ä–∏—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.\n\n"
        "–ê–±–∑–∞—Ü—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –æ–¥–Ω–æ–π –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.\n\n"
        "–ï—Å–ª–∏ place_hashtag –∑–∞–¥–∞–Ω ‚Äî –≤–∫–ª—é—á–∏ –µ–≥–æ –≤ –º–∞—Å—Å–∏–≤ hashtags.\n"
        "–ù–µ –≤—Å—Ç–∞–≤–ª—è–π —Ö—ç—à—Ç–µ–≥–∏ –≤ caption; –≤–µ—Ä–Ω–∏ –∏—Ö —Ç–æ–ª—å–∫–æ –≤ –º–∞—Å—Å–∏–≤–µ hashtags.\n\n"
        '–í–µ—Ä–Ω–∏ JSON: {"caption":"<–¥–≤–∞ –∞–±–∑–∞—Ü–∞>","hashtags":[...]}\n\n'
        "–ü–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º –ø—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: –æ—á–µ–≤–∏–¥–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –∫–æ –≤—Ç–æ—Ä–æ–º—É –∞–±–∑–∞—Ü—É, —Å–≤—è–∑–Ω–æ—Å—Ç—å —Å –ë–∞–ª—Ç–∏–∫–æ–π/—Å—Ü–µ–Ω–æ–π, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è."
    )
    schema = {
        "type": "object",
        "properties": {
            "caption": {"type": "string"},
            "hashtags": {
                "type": "array",
                "items": {"type": "string"},
                "minItems": 2,
            },
        },
        "required": ["caption", "hashtags"],
    }
    attempts = 3
    for attempt in range(1, attempts + 1):
        temperature = bot._creative_temperature()
        attempt_start = time.perf_counter()
        try:
            logging.info(
                "–ó–∞–ø—Ä–æ—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è sea: –º–æ–¥–µ–ª—å=%s temperature=%.2f top_p=0.9 –ø–æ–ø—ã—Ç–∫–∞ %s/%s",
                "gpt-4o",
                temperature,
                attempt,
                attempts,
            )
            bot._enforce_openai_limit(job, "gpt-4o")
            response = await bot.openai.generate_json(
                model="gpt-4o",
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                schema=schema,
                temperature=temperature,
                top_p=0.9,
            )
            attempt_latency = (time.perf_counter() - attempt_start) * 1000
        except Exception:
            attempt_latency = (time.perf_counter() - attempt_start) * 1000
            logging.exception(
                "Failed to generate sea caption (attempt %s) latency_ms=%.1f",
                attempt,
                attempt_latency,
            )
            response = None
        if response:
            await bot._record_openai_usage("gpt-4o", response, job=job)
            finish_reason = "completed"
            if hasattr(response, "meta") and response.meta and isinstance(response.meta, dict):
                finish_reason = response.meta.get("finish_reason", "completed")
            logging.info(
                "SEA_RUBRIC openai_response attempt=%d latency_ms=%.1f finish_reason=%s source=llm",
                attempt,
                attempt_latency,
                finish_reason,
            )
        if not response or not isinstance(response.content, dict):
            logging.warning(
                "SEA_RUBRIC json_parse_error attempt=%d (response missing or not dict)", attempt
            )
            continue
        if "raw" in response.content and "caption" not in response.content:
            logging.warning(
                "SEA_RUBRIC json_parse_error attempt=%d (OpenAI returned raw text, not JSON)",
                attempt,
            )
            continue
        caption_raw = response.content.get("caption")
        raw_hashtags = response.content.get("hashtags")
        if not caption_raw or not isinstance(caption_raw, str):
            logging.warning(
                "SEA_RUBRIC caption_missing_or_invalid attempt=%d (caption not in response or not string)",
                attempt,
            )
            continue
        if not raw_hashtags or not isinstance(raw_hashtags, list):
            logging.warning(
                "SEA_RUBRIC hashtags_missing_or_invalid attempt=%d (using default hashtags)",
                attempt,
            )
            raw_hashtags = []
        cleaned_caption = bot.strip_header(caption_raw)
        caption = cleaned_caption.strip() if cleaned_caption else caption_raw.strip()
        caption, hashtags = bot._build_final_sea_caption(caption, raw_hashtags)
        if not caption:
            logging.warning(
                "SEA_RUBRIC empty_caption_error attempt=%d (caption empty after processing)",
                attempt,
            )
            continue
        paragraphs = [p.strip() for p in caption.split("\n\n") if p.strip()]
        paragraph_count = len(paragraphs)
        if paragraph_count != 2:
            logging.warning(
                "SEA_RUBRIC caption_structure expected 2 paragraphs, got %d", paragraph_count
            )
        if paragraph_count >= 2:
            second_para = paragraphs[1]
            has_lead = any(lead in second_para for lead in leads)
            if not has_lead:
                logging.warning("SEA_RUBRIC caption_leads no standard lead found in paragraph 2")
        caption_length = len(caption)
        if caption_length > 400:
            logging.warning("SEA_RUBRIC caption_length %d exceeds soft limit 400", caption_length)
        if paragraph_count >= 2:
            emoji_pattern = r"[\U0001F300-\U0001F9FF]|[\U0001F600-\U0001F64F]|[\U0001F680-\U0001F6FF]|[\U00002600-\U000027BF]"
            if re.search(emoji_pattern, paragraphs[1]):
                logging.warning(
                    "SEA_RUBRIC caption_emoji found in paragraph 2 (expected only in para 1)"
                )
        if bot._is_duplicate_rubric_copy("sea", "caption", caption, hashtags):
            logging.info(
                "–ü–æ–ª—É—á–µ–Ω –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ç–µ–∫—Å—Ç –¥–ª—è —Ä—É–±—Ä–∏–∫–∏ sea, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞ (%s/%s)",
                attempt,
                attempts,
            )
            continue
        logging.info("SEA_RUBRIC caption_accepted attempt=%d source=llm", attempt)
        return caption, hashtags

    logging.warning(
        "SEA_RUBRIC openai_fallback reason=caption_generation_failed source=fallback attempts=%d",
        attempts,
    )
    raw_fallback = fallback_caption()
    cleaned = bot.strip_header(raw_fallback)
    fallback_text = cleaned.strip() if cleaned else raw_fallback.strip()
    fallback_text = _sanitize_prompt_leaks(fallback_text)
    return fallback_text, default_hashtags


async def generate_sea_caption_with_timeout(
    bot: Bot,
    *,
    storm_state: str,
    storm_persisting: bool,
    wave_height_m: float | None,
    wave_score: float,
    wind_class: str | None,
    wind_ms: float | None,
    wind_kmh: float | None,
    clouds_label: str,
    sunset_selected: bool,
    want_sunset: bool,
    place_hashtag: str | None,
    fact_sentence: str | None,
    now_local_iso: str | None = None,
    day_part: str | None = None,
    tz_name: str | None = None,
    job: Job | None = None,
) -> tuple[str, list[str], dict[str, Any]]:
    import asyncio

    openai_metadata: dict[str, Any] = {
        "openai_calls_per_publish": 0,
        "duration_ms": 0,
        "tokens": 0,
        "retries": 0,
        "timeout_hit": 0,
        "fallback": 0,
    }
    openai_deadline = 90.0
    per_attempt_timeout = 60.0
    max_retries = 1
    backoff_delays = [1.5, 2.0]
    global_start = time.perf_counter()
    for retry_idx in range(max_retries + 1):
        elapsed_global = time.perf_counter() - global_start
        if elapsed_global >= openai_deadline:
            openai_metadata["timeout_hit"] = 1
            openai_metadata["fallback"] = 1
            logging.warning(
                "SEA_RUBRIC OPENAI_CALL timeout=global_deadline elapsed_ms=%.1f",
                elapsed_global * 1000,
            )
            break
        remaining_time = min(per_attempt_timeout, openai_deadline - elapsed_global)
        attempt_start = time.perf_counter()
        try:
            caption_task = asyncio.create_task(
                generate_sea_caption(
                    bot,
                    storm_state=storm_state,
                    storm_persisting=storm_persisting,
                    wave_height_m=wave_height_m,
                    wave_score=wave_score,
                    wind_class=wind_class,
                    wind_ms=wind_ms,
                    wind_kmh=wind_kmh,
                    clouds_label=clouds_label,
                    sunset_selected=sunset_selected,
                    want_sunset=want_sunset,
                    place_hashtag=place_hashtag,
                    fact_sentence=fact_sentence,
                    now_local_iso=now_local_iso,
                    day_part=day_part,
                    tz_name=tz_name,
                    job=job,
                )
            )
            caption, hashtags = await asyncio.wait_for(caption_task, timeout=remaining_time)
            attempt_duration = (time.perf_counter() - attempt_start) * 1000
            openai_metadata["openai_calls_per_publish"] += 1
            openai_metadata["duration_ms"] = round(attempt_duration, 2)
            openai_metadata["retries"] = retry_idx
            logging.info(
                "SEA_RUBRIC OPENAI_CALL success attempt=%d duration_ms=%.1f retries=%d",
                openai_metadata["openai_calls_per_publish"],
                openai_metadata["duration_ms"],
                openai_metadata["retries"],
            )
            return caption, hashtags, openai_metadata
        except TimeoutError:
            attempt_duration = (time.perf_counter() - attempt_start) * 1000
            openai_metadata["openai_calls_per_publish"] += 1
            openai_metadata["timeout_hit"] = 1
            logging.warning(
                "SEA_RUBRIC OPENAI_CALL timeout attempt=%d duration_ms=%.1f timeout_sec=%.1f",
                openai_metadata["openai_calls_per_publish"],
                attempt_duration,
                remaining_time,
            )
            if retry_idx < max_retries:
                openai_metadata["retries"] = retry_idx + 1
                backoff_delay = backoff_delays[min(retry_idx, len(backoff_delays) - 1)]
                logging.info("SEA_RUBRIC OPENAI_CALL retry backoff_sec=%.1f", backoff_delay)
                await asyncio.sleep(backoff_delay)
            else:
                openai_metadata["fallback"] = 1
                break
        except Exception:
            attempt_duration = (time.perf_counter() - attempt_start) * 1000
            openai_metadata["openai_calls_per_publish"] += 1
            logging.exception(
                "SEA_RUBRIC OPENAI_CALL error attempt=%d duration_ms=%.1f",
                openai_metadata["openai_calls_per_publish"],
                attempt_duration,
            )
            if retry_idx < max_retries:
                openai_metadata["retries"] = retry_idx + 1
                backoff_delay = backoff_delays[min(retry_idx, len(backoff_delays) - 1)]
                logging.info("SEA_RUBRIC OPENAI_CALL retry backoff_sec=%.1f", backoff_delay)
                await asyncio.sleep(backoff_delay)
            else:
                openai_metadata["fallback"] = 1
                break
    openai_metadata["fallback"] = 1
    total_duration = (time.perf_counter() - global_start) * 1000
    openai_metadata["duration_ms"] = round(total_duration, 2)
    logging.info(
        "SEA_RUBRIC OPENAI_FALLBACK total_duration_ms=%.1f calls=%d retries=%d timeout_hit=%d source=fallback",
        openai_metadata["duration_ms"],
        openai_metadata["openai_calls_per_publish"],
        openai_metadata["retries"],
        openai_metadata["timeout_hit"],
    )
    leads_fallback = [
        "–ê –≤—ã –∑–Ω–∞–ª–∏",
        "–ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã",
        "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç:",
        "–≠—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:",
        "–ö—Å—Ç–∞—Ç–∏:",
        "–ö —Å–ª–æ–≤—É –æ –ë–∞–ª—Ç–∏–∫–µ,",
        "–¢–µ–ø–µ—Ä—å –≤—ã –±—É–¥–µ—Ç–µ –∑–Ω–∞—Ç—å:",
        "–ù–µ–±–æ–ª—å—à–æ–π —Ñ–∞–∫—Ç:",
        "–ü–æ–¥–µ–ª—é—Å—å —Ñ–∞–∫—Ç–æ–º:",
        "–ö —Å–ª–æ–≤—É",
    ]
    if storm_state == "strong_storm":
        fallback_opening = "–°–µ–≥–æ–¥–Ω—è —Å–∏–ª—å–Ω—ã–π —à—Ç–æ—Ä–º –Ω–∞ –º–æ—Ä–µ ‚Äî –≤–æ–ª–Ω—ã –≥—Ä–µ–º—è—Ç —É —Å–∞–º–æ–≥–æ –±–µ—Ä–µ–≥–∞."
    elif storm_state == "storm":
        if storm_persisting:
            fallback_opening = "–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —à—Ç–æ—Ä–º–∏—Ç—å –Ω–∞ –º–æ—Ä–µ ‚Äî –≤–æ–ª–Ω—ã –≤—Å—ë –µ—â—ë –±—å—é—Ç –æ –±–µ—Ä–µ–≥."
        else:
            fallback_opening = "–°–µ–≥–æ–¥–Ω—è —à—Ç–æ—Ä–º –Ω–∞ –º–æ—Ä–µ ‚Äî –≤–æ–ª–Ω—ã —É–ø—Ä—è–º–æ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –æ –∫—Ä–æ–º–∫—É."
    else:
        fallback_opening = (
            "–ü–æ—Ä–∞–¥—É—é –∑–∞–∫–∞—Ç–æ–º –Ω–∞–¥ –º–æ—Ä–µ–º ‚Äî –ø–æ–±–µ—Ä–µ–∂—å–µ –¥—ã—à–∏—Ç —Ç–µ–ø–ª–æ–º."
            if sunset_selected
            else "–ü–æ—Ä–∞–¥—É—é –≤–∞—Å –º–æ—Ä–µ–º ‚Äî —Ç–∏—Ö–∏–π –±–µ—Ä–µ–≥ –∏ —Ä–æ–≤–Ω—ã–π –ø–ª–µ—Å–∫."
        )
    second_para_fallback = ""
    if fact_sentence:
        lead_fallback = random.choice(leads_fallback)
        fact_text_fallback = fact_sentence.strip()
        if lead_fallback.endswith(":") or lead_fallback.endswith(","):
            second_para_fallback = f"{lead_fallback} {fact_text_fallback}"
        else:
            second_para_fallback = f"{lead_fallback}, {fact_text_fallback}"
    if second_para_fallback:
        fallback_caption = f"{fallback_opening}\n\n{second_para_fallback}"
    else:
        if wind_class == "very_strong":
            fallback_caption = f"{fallback_opening} –í–µ—Ç–µ—Ä —Å—Ä—ã–≤–∞–µ—Ç —à–∞–ø–∫–∏ –Ω–∞ –Ω–∞–±–µ—Ä–µ–∂–Ω–æ–π."
        elif wind_class == "strong":
            fallback_caption = f"{fallback_opening} –í–µ—Ç–µ—Ä –æ—â—É—Ç–∏–º–æ —Ç—è–Ω–µ—Ç –∫ –º–æ—Ä—é."
        elif storm_state == "calm":
            fallback_caption = f"{fallback_opening} –ù–∞ –ø–æ–±–µ—Ä–µ–∂—å–µ —Å–ø–æ–∫–æ–π–Ω–æ –∏ —Ö–æ—á–µ—Ç—Å—è –∑–∞–¥–µ—Ä–∂–∞—Ç—å—Å—è."
        else:
            fallback_caption = fallback_opening
    fallback_caption = _sanitize_prompt_leaks(fallback_caption)
    return fallback_caption, bot._default_hashtags("sea"), openai_metadata
